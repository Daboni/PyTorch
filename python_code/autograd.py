# -*- coding: utf-8 -*-
"""AUTOGRAD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZpZTbGi-6vgcJ86YdUgxqJ65v5h63977

AUTOGRAD 자동미분
"""

import torch

"""requires_grad = True 를 설정하여 연산을 기록"""

x = torch.ones(2,2,requires_grad=True)
print(x)

y = x+2
print(y)

z = y*y*3
out = z.mean()

print(z,out)

""".requires.grad_() 은 기존 tensor의 requires.grad 값을 바꿔치기 하여 변경"""

x = torch.randn(2,2)
x = (x*3)/(x-1)
print(x.requires_grad)
x.requires_grad_(True)
print(x.requires_grad)
y = (x*x).sum()
print(y.grad_fn)

"""Gradient : 변화도"""

out.backward()

print(x)3
print(x.grad_fn)
print(x.grad)

"""torch.autograd 는 벡터-야코비안 곱을 계산하는 엔진"""

x = torch.randn(3, requires_grad=True)

y = x*2
while y.data.norm() < 1000:
  y = y*2

print(y)

print(x)

v=torch.tensor([0.1,1.0,0.0001],dtype=torch.float)
y.backward(v)

print(x.grad)

"""with torch.no_grad(): 를 통하여 autograd가 .requires_grad = True 인 Tensor들의 연산 기록을 추적하는 것을 멈출 수 있음."""

print(x.requires_grad)
print((x**2).requires_grad)

with torch.no_grad() : 
  print((x**2).requires_grad)

""".detach()를 이용하여 content는 같지만 requires_grad가 다른 새로운 tensor를 가져올 수 있음"""

print(x.requires_grad)
y=x.detach()
print(y.requires_grad)
print(x.eq(y).all())

x = torch.randn(2,2)
print(x.requires_grad)
y=x.detach()
print(y.requires_grad)
print(x.eq(y).all())

x = torch.ones(2,2)
y = torch.ones(2,2)
print(x.eq(y).all())

